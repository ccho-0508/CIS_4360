### One Pager: How Twitter Built a Metering & Chargeback System to Incentivize Higher Resource Utilization of Twitter Infrastructure


Twitter was initially known as “The Monorail” and it was one of the largest Ruby on Rails deployments. At its peak, the application could service 3283 tweets per second (TPS) which was 5 times the average rate. The issue with this was the unpredictability of events that would cause a spike in TPS. Whenever viral events would happen, the Monorail would go down, marking it as unreliable and unproductive. The next step was adapting the application to adhere to a service oriented architecture, specifically by using microservices. By re-architecting the stack, there were many benefits such as fault tolerance, isolated teams, scalable and efficient services, and increased developer productivity.

I had a good understanding of these benefits thanks to the readings and labs from class, but the challenges that inspired Twitter’s chargeback system were very unique problems I hadn’t thought of. The “Innovation Insight for Microservices” paper from Gartner did touch on these issues in the section, 'The Costs of Microservices Adoption'. They list the ways microservices disrupt many aspects of an organization, and many of them are touched on in the video. The two that stood out were :

> - “Data management — Microservices patterns fundamentally change how data is managed within an application, which impacts data governance, reporting and analytics practices.” 
> - “Product mindset — MSA requires a product mindset, which is an organizational change that affects budgeting, staffing and planning outside of IT. It impacts funding models, roles and responsibilities, development team structures, and corporate communication structures”. 

These 'disruptions' are clear with the need for a chargeback system (product mindset), and the four challenges (data management) faced when designing it : identifying services, cataloging resources, metering and chargebacks, and service metadata. By abstracting away the differences in provisioning services, resources, and metrics about resource usage, these complex services are organized in a way that simplifies the chargeback process and gives a more detailed report of expenses across all teams. It was interesting to hear about the reasons for these issues and see how they were resolved and combined to create the chargeback system. 

Although Twitter has switched from Mesos to Kubernetes, Mesos was an integral part of the infrastructure as the orchestration system for scheduling and managing clusters. The chargeback system was built on Mesos, so it could track its containers/services. For the case study I would look further into Mesos to understand how it works in comparison to Kubernetes and why the switch was made. 

